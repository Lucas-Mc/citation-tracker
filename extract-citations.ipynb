{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 1. PubMed\n",
    "\n",
    "Search PubMed for papers\n",
    "\n",
    "https://www.ncbi.nlm.nih.gov/pubmed/\n",
    "\n",
    "https://www.ncbi.nlm.nih.gov/books/NBK25499/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import os\n",
    "from textwrap import wrap\n",
    "\n",
    "from Bio import Entrez\n",
    "from IPython.display import display\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import source.reuse as reuse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENTREZ_EMAIL = 'lumccul3@gmail.com'\n",
    "\n",
    "# base_query = '(\"Mathematical Concepts\"[Mesh] OR \"Operations Research\"[Mesh] OR \"Game Theory\"[Mesh] OR \"Markov Chains\"[Mesh] OR \"Heuristics\"[Mesh] OR \"robust optimization\" OR \"queuing systems\" OR \"operations research\" OR \"operational research\" OR \"markov decision process\" OR \"stochastic program\" OR \"stochastic processes\" OR \"combinatorial optimization\" OR \"discrete optimization\" OR \"approximation algorithms\" OR \"heuristics\" OR \"dynamic program\" OR \"dynamic programming\" OR \"linear program\" OR \"linear programming\" OR \"integer program\" OR \"integer programming\" OR \"mixed-integer program\" OR \"mixed-integer programming\" OR \"stochastic optimization\" OR \"convex optimization\" OR \"quadratic optimization\" OR \"quadratic program\" OR \"quadratic programming\" OR \"non-smooth optimization\" OR \"non-convex optimization\" OR \"multicriteria optimization\" OR \"goal programming\" OR \"queuing theory\" OR \"game theory\" OR \"tabu search\" OR \"genetic algorithm\" OR \"simulated annealing\" OR \"variable neighborhood search\" OR \"ant colony\") AND (\"Clinical Decision-Making\"[Mesh] OR \"Decision Support Techniques\"[Mesh] OR \"Decision Support Systems, Clinical\"[Mesh] OR \"Decision Making\"[Mesh] OR \"Decision Theory\"[Mesh] OR \"Clinical Decision Rules\"[Mesh] OR \"Decision Trees\"[Mesh] OR \"Cost-Benefit Analysis\"[Mesh] OR \"decision\") AND (\"Radiotherapy\"[Mesh]) NOT \"Radiotherapy Setup Errors\"[Mesh] NOT \"Uncertainty\"[Mesh] NOT \"Disaster Planning\"[Mesh] NOT \"Meta-Analysis\" [Publication Type] NOT \"Bionics\"[Mesh] NOT \"Quality Assurance, Health Care\"[Mesh] NOT \"Cells, Cultured\"[Mesh] NOT \"Survival Analysis\"[Mesh] NOT \"Quality Control\"[Mesh] NOT \"Retrospective Studies\"[Mesh] NOT \"Observational Study\"[Publication Type] NOT \"Phantoms, Imaging\"[Mesh] NOT \"Radiation Protection\"[Mesh] NOT \"Cryopreservation\"[Mesh] NOT \"Radiometry\"[Mesh] NOT \"Clinical Trial\"[Publication Type] NOT \"Software\"[Mesh] NOT \"Legal Case\"[Publication Type] NOT \"Anisotropy\"[Mesh] NOT \"Diagnosis, Differential\"[Mesh] NOT \"Patient Positioning\"[Mesh] NOT \"Radiology, Interventional\"[Mesh] NOT \"Algorithms\"[Mesh]'\n",
    "# extra_query = '(\"Mathematical Concepts\"[Mesh] OR \"Operations Research\"[Mesh] OR \"Game Theory\"[Mesh] OR \"Markov Chains\"[Mesh] OR \"Heuristics\"[Mesh] OR \"robust optimization\" OR \"queuing systems\" OR \"operations research\" OR \"operational research\" OR \"markov decision process\" OR \"stochastic program\" OR \"stochastic processes\" OR \"combinatorial optimization\" OR \"discrete optimization\" OR \"approximation algorithms\" OR \"heuristics\" OR \"dynamic program\" OR \"dynamic programming\" OR \"linear program\" OR \"linear programming\" OR \"integer program\" OR \"integer programming\" OR \"mixed-integer program\" OR \"mixed-integer programming\" OR \"stochastic optimization\" OR \"convex optimization\" OR \"quadratic optimization\" OR \"quadratic program\" OR \"quadratic programming\" OR \"non-smooth optimization\" OR \"non-convex optimization\" OR \"multicriteria optimization\" OR \"goal programming\" OR \"queuing theory\" OR \"game theory\" OR \"tabu search\" OR \"genetic algorithm\" OR \"simulated annealing\" OR \"variable neighborhood search\" OR \"ant colony\") AND (\"Clinical Decision-Making\"[Mesh] OR \"Decision Support Techniques\"[Mesh] OR \"Decision Support Systems, Clinical\"[Mesh] OR \"Decision Making\"[Mesh] OR \"Decision Theory\"[Mesh] OR \"Clinical Decision Rules\"[Mesh] OR \"Decision Trees\"[Mesh] OR \"Cost-Benefit Analysis\"[Mesh] OR \"decision\") AND (\"Radiotherapy\"[Mesh] OR \"radiotherapy\") NOT \"Radiotherapy Setup Errors\"[Mesh] NOT \"Uncertainty\"[Mesh] NOT \"Disaster Planning\"[Mesh] NOT \"Meta-Analysis\" [Publication Type] NOT \"Bionics\"[Mesh] NOT \"Quality Assurance, Health Care\"[Mesh] NOT \"Cells, Cultured\"[Mesh] NOT \"Survival Analysis\"[Mesh] NOT \"Quality Control\"[Mesh] NOT \"Retrospective Studies\"[Mesh] NOT \"Observational Study\"[Publication Type] NOT \"Phantoms, Imaging\"[Mesh] NOT \"Radiation Protection\"[Mesh] NOT \"Cryopreservation\"[Mesh] NOT \"Radiometry\"[Mesh] NOT \"Clinical Trial\"[Publication Type] NOT \"Software\"[Mesh] NOT \"Legal Case\"[Publication Type] NOT \"Anisotropy\"[Mesh] NOT \"Diagnosis, Differential\"[Mesh] NOT \"Patient Positioning\"[Mesh] NOT \"Radiology, Interventional\"[Mesh] NOT \"DNA\"[Mesh] NOT \"Radiotherapy Planning, Computer-Assisted\"[Mesh] NOT \"Machine Learning\"[Mesh] NOT \"Artificial Intelligence\"[Mesh] NOT \"Prognosis\"[Mesh]'\n",
    "\n",
    "base_query = '(\"Stochastic Processes\"[Mesh] OR \"Operations Research\"[Mesh] OR \"operations research\" OR \"operational research\" OR \"Markov Model\" OR \"Markov models\" OR \"Markov chain\" OR \"Markov chains\" OR \"Markov decision\") AND (\"Radiotherapy\"[Mesh] OR \"radiotherapy\" OR \"radiation therapy\") AND (\"Clinical Decision-Making\"[Mesh] OR \"Decision Support Techniques\"[Mesh] OR \"Decision Support Systems, Clinical\"[Mesh] OR \"Decision Making\"[Mesh] OR \"Decision Theory\"[Mesh] OR \"Clinical Decision Rules\"[Mesh] OR \"Decision Trees\"[Mesh] OR \"decision\") NOT \"Anisotropy\"[Mesh] NOT \"Artificial Intelligence\"[Mesh] NOT \"Bionics\"[Mesh] NOT \"Cells, Cultured\"[Mesh] NOT \"Clinical Trial\"[Publication Type] NOT \"Cryopreservation\"[Mesh] NOT \"Diagnosis, Differential\"[Mesh] NOT \"Disaster Planning\"[Mesh] NOT \"DNA\"[Mesh] NOT \"Legal Case\"[Publication Type] NOT \"Machine Learning\"[Mesh] NOT \"Meta-Analysis\" [Publication Type] NOT \"Observational Study\"[Publication Type] NOT \"Patient Positioning\"[Mesh] NOT \"Phantoms, Imaging\"[Mesh] NOT \"Radiation Protection\"[Mesh] NOT \"Radiology, Interventional\"[Mesh] NOT \"Radiometry\"[Mesh] NOT \"Radiotherapy Planning, Computer-Assisted\"[Mesh] NOT \"Radiotherapy Setup Errors\"[Mesh] NOT \"Retrospective Studies\"[Mesh] NOT \"Software\"[Mesh] NOT \"Survival Analysis\"[Mesh] NOT \"Quality Assurance, Health Care\"[Mesh] NOT \"Quality Control\"[Mesh] AND \"English\" [LA] AND 2000:2022[dp]'\n",
    "extra_query = '(\"Stochastic Processes\"[Mesh] OR \"Operations Research\"[Mesh] OR \"operations research\" OR \"operational research\" OR \"Markov Model\" OR \"Markov models\" OR \"Markov chain\" OR \"Markov chains\") AND (\"Radiotherapy\"[Mesh] OR \"radiotherapy\" OR \"radiation therapy\") AND (\"Clinical Decision-Making\"[Mesh] OR \"Decision Support Techniques\"[Mesh] OR \"Decision Support Systems, Clinical\"[Mesh] OR \"Decision Making\"[Mesh] OR \"Decision Theory\"[Mesh] OR \"Clinical Decision Rules\"[Mesh] OR \"Decision Trees\"[Mesh] OR \"decision\") NOT \"Anisotropy\"[Mesh] NOT \"Artificial Intelligence\"[Mesh] NOT \"Bionics\"[Mesh] NOT \"Cells, Cultured\"[Mesh] NOT \"Clinical Trial\"[Publication Type] NOT \"Cryopreservation\"[Mesh] NOT \"Diagnosis, Differential\"[Mesh] NOT \"Disaster Planning\"[Mesh] NOT \"DNA\"[Mesh] NOT \"Legal Case\"[Publication Type] NOT \"Machine Learning\"[Mesh] NOT \"Meta-Analysis\" [Publication Type] NOT \"Observational Study\"[Publication Type] NOT \"Patient Positioning\"[Mesh] NOT \"Prognosis\"[Mesh] NOT \"Phantoms, Imaging\"[Mesh] NOT \"Radiation Protection\"[Mesh] NOT \"Radiology, Interventional\"[Mesh] NOT \"Radiometry\"[Mesh] NOT \"Radiotherapy Planning, Computer-Assisted\"[Mesh] NOT \"Radiotherapy Setup Errors\"[Mesh] NOT \"Retrospective Studies\"[Mesh] NOT \"Software\"[Mesh] NOT \"Survival Analysis\"[Mesh] NOT \"Quality Assurance, Health Care\"[Mesh] NOT \"Quality Control\"[Mesh] AND \"English\" [LA] AND 2000:2022[dp]'\n",
    "\n",
    "# refined_query = '(\"Stochastic Processes\"[Mesh] OR \"Operations Research\"[Mesh] OR \"operations research\" OR \"operational research\" OR \"Markov Model\" OR \"Markov models\" OR \"Markov chain\" OR \"Markov chains\") AND (\"Radiotherapy\"[Mesh] OR \"radiotherapy\" OR \"radiation therapy\") AND (\"Clinical Decision-Making\"[Mesh] OR \"Decision Support Techniques\"[Mesh] OR \"Decision Support Systems, Clinical\"[Mesh] OR \"Decision Making\"[Mesh] OR \"Decision Theory\"[Mesh] OR \"Clinical Decision Rules\"[Mesh] OR \"Decision Trees\"[Mesh] OR \"decision\") NOT \"Anisotropy\"[Mesh] NOT \"Artificial Intelligence\"[Mesh] NOT \"Bionics\"[Mesh] NOT \"Cells, Cultured\"[Mesh] NOT \"Clinical Trial\"[Publication Type] NOT \"Cryopreservation\"[Mesh] NOT \"Diagnosis, Differential\"[Mesh] NOT \"Disaster Planning\"[Mesh] NOT \"DNA\"[Mesh] NOT \"Legal Case\"[Publication Type] NOT \"Machine Learning\"[Mesh] NOT \"Meta-Analysis\" [Publication Type] NOT \"Observational Study\"[Publication Type] NOT \"Patient Positioning\"[Mesh] NOT \"Prognosis\"[Mesh] NOT \"Phantoms, Imaging\"[Mesh] NOT \"Radiation Protection\"[Mesh] NOT \"Radiology, Interventional\"[Mesh] NOT \"Radiometry\"[Mesh] NOT \"Radiotherapy Planning, Computer-Assisted\"[Mesh] NOT \"Radiotherapy Setup Errors\"[Mesh] NOT \"Retrospective Studies\"[Mesh] NOT \"Software\"[Mesh] NOT \"Survival Analysis\"[Mesh] NOT \"Quality Assurance, Health Care\"[Mesh] NOT \"Quality Control\"[Mesh] NOT \"Uncertainty\"[Mesh] AND \"English\" [LA] AND 2000:2022[dp]'\n",
    "\n",
    "search_strings = [\n",
    "    base_query,\n",
    "    extra_query\n",
    "    # refined_query\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_results_all = reuse.search_list(search_strings, ENTREZ_EMAIL, all=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display number of results\n",
    "for ss in search_strings:\n",
    "    result = search_results_all[ss]\n",
    "    print(f'{ss}:\\n - Count: {len(result.index)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out to get the new URLs\n",
    "all_urls = [list(search_results_all[k]['URL'].values) for k in search_results_all.keys()]\n",
    "\n",
    "# If a new restriction is just added\n",
    "all_urls = [i for sl in all_urls for i in sl]\n",
    "new_urls = [l for l,c in dict(Counter(all_urls)).items() if c == 1]\n",
    "new_pubs = search_results_all[list(search_results_all.keys())[0]]\n",
    "\n",
    "# If a totally new search to add new publications\n",
    "# new_urls = [l for l in all_urls[1] if l not in all_urls[0]]\n",
    "# new_pubs = search_results_all[list(search_results_all.keys())[-1]]\n",
    "\n",
    "new_pubs = new_pubs[new_pubs['URL'].isin(new_urls)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the titles to files\n",
    "write_dir = os.path.join('search_results', 'pubmed')\n",
    "os.makedirs(write_dir, exist_ok=True)\n",
    "\n",
    "new_pubs.to_csv(os.path.join(write_dir, 'new-publications_refined-revised.csv'), index=False)\n",
    "\n",
    "# search_results_all[base_query].to_csv(os.path.join(write_dir, 'without-constraints-all.csv'), index=False)\n",
    "# search_results_all[full_query].to_csv(os.path.join(write_dir, 'with-constraints-all.csv'), index=False)\n",
    "\n",
    "# base_query_file = os.path.join(write_dir, 'without-constraints.txt')\n",
    "# with open(base_query_file, 'w') as f:\n",
    "#     for line in search_results[base_query].paper_titles:\n",
    "#         f.write(line+'\\n')\n",
    "\n",
    "# full_query_file = os.path.join(write_dir, 'with-constraints.txt')\n",
    "# with open(full_query_file, 'w') as f:\n",
    "#     for line in search_results[full_query].paper_titles:\n",
    "#         f.write(line+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new one for each year\n",
    "all_years = sorted(set(list(search_results_all[base_query]['Publication Year'])))\n",
    "# all_years = sorted(set(list(search_results_all[full_query]['Publication Year'])))\n",
    "for year in all_years:\n",
    "    year_df_without = search_results_all[base_query][search_results_all[base_query]['Publication Year'] == year]\n",
    "    # year_df_with = search_results_all[full_query][search_results_all[full_query]['Publication Year'] == year]\n",
    "    year_df_without.to_csv(os.path.join(write_dir, f'without-constraints-all_{year}.csv'), index=False)\n",
    "    # year_df_with.to_csv(os.path.join(write_dir, f'with-constraints-all_{year}.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a histogram of number of publications each year\n",
    "all_years = [int(y) for y in sorted(set(list(search_results_all[base_query]['Publication Year'])))]\n",
    "# all_years = [int(y) for y in sorted(set(list(search_results_all[full_query]['Publication Year'])))]\n",
    "\n",
    "year_without = []\n",
    "year_with = []\n",
    "interval_years = range(min(all_years), max(all_years)+1)\n",
    "for year in interval_years:\n",
    "    year_df_without = search_results_all[base_query][search_results_all[base_query]['Publication Year'] == str(year)]\n",
    "    # year_df_with = search_results_all[full_query][search_results_all[full_query]['Publication Year'] == str(year)]\n",
    "\n",
    "    for _ in range(len(year_df_without.index)):\n",
    "        year_without.append(year)\n",
    "    # for _ in range(len(year_df_with.index)):\n",
    "    #     year_with.append(year)\n",
    "\n",
    "plt.figure(figsize=(10,8))\n",
    "h = plt.hist(year_without, facecolor='k', edgecolor='w', bins=np.arange(min(interval_years)-1, max(interval_years)+5)-0.5)\n",
    "plt.xlabel('Year', fontsize=12)\n",
    "plt.ylabel('Number of Publications', fontsize=12)\n",
    "plt.ylim([0, 1.05*max(h[0])])\n",
    "plt.title('\\n'.join(wrap(search_strings[0], 140)), fontsize=8)\n",
    "plt.annotate(f'N = {len(year_without)}', xy=(0.05,0.9), xytext=(0.05,0.9), xycoords='axes fraction', size=16)\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(write_dir, 'without-constraints_histogram.jpg'))\n",
    "plt.show()\n",
    "\n",
    "# plt.figure(figsize=(10,6))\n",
    "# plt.hist(year_with, facecolor='k', edgecolor='w', bins=np.arange(min(interval_years)-1, max(interval_years)+5)-0.5)\n",
    "# plt.xlabel('Year', fontsize=12)\n",
    "# plt.ylabel('Number of Publications', fontsize=12)\n",
    "# plt.ylim([0, 1.05*max(h[0])])\n",
    "# plt.title('\\n'.join(wrap(search_strings[-1], 140)), fontsize=8)\n",
    "# plt.savefig(os.path.join(write_dir, 'with-constraints_histogram.jpg'))\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign authors to articles to review\n",
    "CANCER_AUTHORS = ['Lucas', 'Cem']\n",
    "OPTIM_AUTHORS = ['Soheil', 'Mohammad', 'Aysenur']\n",
    "ALL_AUTHORS = [*CANCER_AUTHORS, *OPTIM_AUTHORS]\n",
    "\n",
    "# Create the dataframe from a subset of the original\n",
    "assignment_file = new_pubs[['Title', 'URL']]\n",
    "# assignment_file = search_results_all[base_query][['Title', 'URL']]\n",
    "# assignment_file = search_results_all[full_query][['Title', 'URL']]\n",
    "total_publications = len(assignment_file.index)\n",
    "\n",
    "# Cycle through the authors to make it even\n",
    "optim_index = 0\n",
    "cancer_index = 0\n",
    "reviewer1s = []\n",
    "reviewer2s = []\n",
    "for p in range(total_publications):\n",
    "    # Cycle through all authors\n",
    "    reviewer1s.append(ALL_AUTHORS[p%len(ALL_AUTHORS)])\n",
    "    # Match authors with their opposite domain\n",
    "    if reviewer1s[p] in CANCER_AUTHORS:\n",
    "        reviewer2s.append(OPTIM_AUTHORS[optim_index%len(OPTIM_AUTHORS)])\n",
    "        optim_index += 1\n",
    "    elif reviewer1s[p] in OPTIM_AUTHORS:\n",
    "        reviewer2s.append(CANCER_AUTHORS[cancer_index%len(CANCER_AUTHORS)])\n",
    "        cancer_index += 1\n",
    "# Append to the dataframe\n",
    "assignment_file['Reviewer1'] = reviewer1s\n",
    "assignment_file['Decision1'] = total_publications*['']\n",
    "assignment_file['Comments1'] = total_publications*['']\n",
    "assignment_file['Reviewer2'] = reviewer2s\n",
    "assignment_file['Decision2'] = total_publications*['']\n",
    "assignment_file['Comments2'] = total_publications*['']\n",
    "assignment_file = assignment_file.sort_values(by=['Reviewer1'])\n",
    "\n",
    "assignment_file.to_csv(os.path.join(write_dir, f'new-publications_assignment-file.csv'), index=False)\n",
    "# assignment_file.to_csv(os.path.join(write_dir, f'without-constraints_assignment-file.csv'), index=False)\n",
    "# assignment_file.to_csv(os.path.join(write_dir, f'with-constraints_assignment-file.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Total\n",
    "print('Total:\\n')\n",
    "print('Author   | Reviewer 1 | Reviewer 2')\n",
    "print('---------|------------|-----------')\n",
    "for a in ALL_AUTHORS:\n",
    "    c1 = reviewer1s.count(a)\n",
    "    c2 = reviewer2s.count(a)\n",
    "    print(f'{a:8} | {c1:^10} | {c2:^10}')\n",
    "\n",
    "# Combinations\n",
    "print('\\nCombinations:\\n')\n",
    "print(dict(Counter(tuple(sorted(tup)) for tup in list(zip(reviewer1s, reviewer2s)))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read in results and filter by publication year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file = os.path.join('search_results', 'pubmed', 'review_filtered_04-23-2022.xlsx')\n",
    "review_results = pd.read_excel(input_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pmids = [int(r.split('/')[-2]) for r in review_results['URL'].to_list()]\n",
    "Entrez.email = ENTREZ_EMAIL\n",
    "\n",
    "all_years = []\n",
    "for pmid in pmids:\n",
    "    handle = Entrez.efetch(db='pubmed', retmode='xml', id=pmid)\n",
    "    results = Entrez.read(handle)\n",
    "    try:\n",
    "        all_years.append(results['PubmedArticle'][0]['MedlineCitation']['DateCompleted']['Year'])\n",
    "    except TypeError:\n",
    "        print(results)\n",
    "plt.hist(all_years)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at the effect of restricting the search by additional criteria.\n",
    "# The differences show that many false positives, and a few true positives, are removed.\n",
    "reuse.showdiff(search_results_all[search_strings[0]],\n",
    "               search_results_all[search_strings[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the labelled results for the general unconstrained search query\n",
    "labelled_results = pd.read_csv(os.path.join(write_dir, 'without-constraints-inspected.tsv'), delimiter='\\t', header=None)\n",
    "false_positives = labelled_results.loc[labelled_results[1]=='F'][0].values\n",
    "true_positives = labelled_results.loc[labelled_results[1]=='T'][0].values\n",
    "print('Number of results found using the unconstrained search term:', len(labelled_results))\n",
    "print('Number of false positives:', len(false_positives))\n",
    "print('Number of true positives:', len(true_positives))\n",
    "\n",
    "constrained_titles = search_results[search_strings[1]].paper_titles\n",
    "print('\\nCompare ^ true positives with:')\n",
    "print('Number of results from the constrained search term:', len(constrained_titles))\n",
    "\n",
    "missed_papers = set(true_positives) - set(constrained_titles)\n",
    "print('Number of missed true positives:', len(missed_papers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take a look at some True positives missed by the constrained search term to figure out what else you can add.\n",
    "# Write to a file to label comments.\n",
    "write_dir = os.path.join('search_results', 'pubmed')\n",
    "\n",
    "missed_papers_file = os.path.join(write_dir, 'missed-papers.tsv')\n",
    "\n",
    "with open(missed_papers_file, 'w') as f:\n",
    "    for line in missed_papers:\n",
    "        f.write(line+'\\n')\n",
    "\n",
    "display(missed_papers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Web of Science"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import re\n",
    "import time\n",
    "from IPython.display import display\n",
    "\n",
    "\n",
    "base_mimic_query = '(mimic-ii OR mimic-iii OR mimicii OR mimiciii OR mimic-2 OR mimic-3 OR mimic2 OR mimic3)'\n",
    "restriction_query = '(physionet OR icu OR “intensive care” OR “critical care”)'\n",
    "def full_query(base_query, restriction_query):\n",
    "    return ' AND '.join([base_query, restriction_query])\n",
    "full_mimic_query = full_query(base_mimic_query, restriction_query)\n",
    "#base_search_url = 'https://apps.webofknowledge.com/WOS_GeneralSearch_input.do?product=WOS&search_mode=GeneralSearch&SID=2F46AeWkMQBRAZlzDWm&preferencesSaved='\n",
    "base_search_url = 'https://apps.webofknowledge.com/WOS_GeneralSearch_input.do?product=WOS&search_mode=GeneralSearch&SID=1AnC2UMojuKrtrl7T5R&preferencesSaved='\n",
    "\n",
    "all_titles = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get to the search page\n",
    "driver = webdriver.Firefox()\n",
    "driver.get(base_search_url)\n",
    "\n",
    "# Input the query string\n",
    "time.sleep(2.5)\n",
    "searchbox = driver.find_element_by_id('value(input1)')\n",
    "searchbox.send_keys(full_mimic_query)\n",
    "\n",
    "# Search\n",
    "time.sleep(1)\n",
    "searchbutton = driver.find_element_by_css_selector('.standard-button.primary-button.large-search-button')\n",
    "searchbutton.click()\n",
    "\n",
    "# Get the total number of pages\n",
    "npages = int(driver.find_element_by_id('pageCount.top').text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the titles!!!\n",
    "while True:\n",
    "    # Get the current page number\n",
    "    pagenum = int(driver.find_element_by_class_name('goToPageNumber-input').get_property('value'))\n",
    "\n",
    "    # Get the titles. This also captures the journals. So every second value is not a title.\n",
    "    elements = driver.find_elements_by_class_name('smallV110')\n",
    "\n",
    "    for e in elements[::2]:\n",
    "        all_titles.append(e.find_element_by_tag_name('value').text)\n",
    "        \n",
    "    if pagenum < npages:\n",
    "        nextbutton = driver.find_element_by_class_name('paginationNext')\n",
    "        nextbutton.click()\n",
    "    else:\n",
    "        print('Got all paper titles!')\n",
    "        driver.close()\n",
    "        break\n",
    "        \n",
    "all_titles = set(all_titles)\n",
    "#all_titles.remove('')\n",
    "all_titles = [t.lower() for t in list(all_titles)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(all_titles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the titles to files\n",
    "write_dir = os.path.join('search_results/wos')\n",
    "\n",
    "full_query_file = os.path.join(write_dir, 'with-constraints.txt')\n",
    "\n",
    "with open(full_query_file, 'w') as f:\n",
    "    for line in all_titles:\n",
    "        f.write(line+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. SCOPUS\n",
    "\n",
    "Shit search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. IEEE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import re\n",
    "import time\n",
    "from IPython.display import display\n",
    "import os\n",
    "\n",
    "base_mimic_query = '(mimic-ii OR mimic-iii OR mimicii OR mimiciii OR mimic-2 OR mimic-3 OR mimic2 OR mimic3)'\n",
    "restriction_query = '(physionet OR icu OR “intensive care” OR “critical care”)'\n",
    "def full_query(base_query, restriction_query):\n",
    "    return ' AND '.join([base_query, restriction_query])\n",
    "full_mimic_query = full_query(base_mimic_query, restriction_query)\n",
    "base_search_url = 'http://ieeexplore.ieee.org/search/advsearch.jsp?expression-builder'\n",
    "all_titles = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get to the search page\n",
    "driver = webdriver.Firefox()\n",
    "driver.get(base_search_url)\n",
    "\n",
    "# Input the query string\n",
    "searchbox = driver.find_element_by_id('expression-textarea')\n",
    "searchbox.send_keys(full_mimic_query)\n",
    "# Select the 'full text and metadata' box\n",
    "radiobutton = driver.find_element_by_id('Search_All_Text')\n",
    "radiobutton.click()\n",
    "\n",
    "# Search\n",
    "time.sleep(1)\n",
    "searchbutton = driver.find_element_by_class_name('stats-Adv_Command_search')\n",
    "searchbutton.click()\n",
    "\n",
    "# Get the total number of pages\n",
    "#npages = int(driver.find_element_by_id('pageCount.top').text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the titles!!!\n",
    "while True:\n",
    "    # let the page load\n",
    "    time.sleep(2)\n",
    "    # Get scroll height\n",
    "    last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "    while True:\n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "        time.sleep(0.5)\n",
    "        new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "        if new_height == last_height:\n",
    "            break\n",
    "        last_height = new_height\n",
    "        \n",
    "    # Get the titles.\n",
    "    # They are in: <h2 class=\"result-item-title\"><a class=\"ng-binding ng-scope\">title</a></h2>\n",
    "    elements = driver.find_elements_by_class_name('result-item-title')\n",
    "    for e in elements:\n",
    "        # Text may appear with \"[::sometext::]\"\n",
    "        all_titles.append(e.find_element_by_tag_name('a').get_attribute('text').replace('[::', '').replace('::]', ''))\n",
    "        # New line separated journal info and such\n",
    "        #all_titles.append(e.text.split('\\n')[0])\n",
    "    \n",
    "    # Click next page if any\n",
    "    \n",
    "    e = driver.find_element_by_class_name('next')\n",
    "    if 'disabled' in e.get_attribute('class'):\n",
    "        print('Got all paper titles!')\n",
    "        driver.close()\n",
    "        break\n",
    "    else:\n",
    "        nextbutton = driver.find_element_by_link_text('>')\n",
    "        nextbutton.click()\n",
    "\n",
    "all_titles = set(all_titles)\n",
    "all_titles = [t.lower() for t in list(all_titles)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(all_titles))\n",
    "display(all_titles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the titles to files\n",
    "write_dir = os.path.join('search_results/ieee')\n",
    "\n",
    "full_query_file = os.path.join(write_dir, 'with-constraints.txt')\n",
    "\n",
    "with open(full_query_file, 'w') as f:\n",
    "    for line in all_titles:\n",
    "        f.write(line+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combining Results - pubmed, wos, ieee"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_dir = 'search_results'\n",
    "\n",
    "combined_results = []\n",
    "for service in ['pubmed', 'wos', 'ieee']:\n",
    "    # For pubmed, get the curated true positives from the unconstrained search instead\n",
    "    if service == 'pubmed':\n",
    "        df = pd.read_csv(os.path.join(result_dir, service, 'without-constraints-inspected.tsv'), delimiter='\\t', header=None)\n",
    "        service_results = list(df.loc[df[1]=='T'][0].values)\n",
    "    # For other services, get the constrained search results\n",
    "    else:\n",
    "        with open(os.path.join(result_dir, service, 'with-constraints.txt')) as f:\n",
    "            service_results = f.readlines()\n",
    "    print('Number of results from service '+service+': '+str(len(service_results)))\n",
    "    combined_results = combined_results + [r.strip() for r in service_results]\n",
    "\n",
    "print('\\nTotal number of non-unique results: ', len(combined_results))\n",
    "combined_results = sorted(list(set(combined_results)))\n",
    "print('Total number of unique results: ', len(combined_results))\n",
    "with open(os.path.join(result_dir, 'combined', 'with-constraints.txt'), 'w') as f:\n",
    "    for r in combined_results:\n",
    "        f.write(r+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# may 21 2018\n",
    "Number of results from service pubmed: 155\n",
    "Number of results from service wos: 152\n",
    "Number of results from service ieee: 322\n",
    "\n",
    "Total number of non-unique results:  629\n",
    "Total number of unique results:  456\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attempting to parse GS automatically failed. Below is evidence of failure. Can ignore..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# N. Search Google Scholar\n",
    "\n",
    "Packages found online:\n",
    "- https://github.com/ckreibich/scholar.py\n",
    "- https://github.com/venthur/gscholar\n",
    "- https://github.com/adeel/google-scholar-scraper\n",
    "- http://code.activestate.com/recipes/523047-search-google-scholar/\n",
    "- https://github.com/erdiaker/torrequest\n",
    "- https://github.com/NikolaiT/GoogleScraper\n",
    "\n",
    "\n",
    "- https://stackoverflow.com/questions/8049520/web-scraping-javascript-page-with-python\n",
    "\n",
    "\n",
    "Query: `(\"mimic ii\" OR \"mimic iii\") AND (\"database\" OR \"clinical\" OR \"waveform\" OR ICU)`\n",
    "\n",
    "https://scholar.google.com/scholar?q=%28mimic-ii+OR+mimic-iii%29&btnG=&hl=en&as_sdt=1%2C22&as_vis=1\n",
    "\n",
    "https://scholar.google.com/scholar/help.html\n",
    "\n",
    "\n",
    "https://superuser.com/questions/565722/how-to-config-tor-to-use-a-http-socks-proxy\n",
    "\n",
    "## Requirements\n",
    "\n",
    "1. Browse with JS enabled. requests library uses http. Otherwise google will think (correctly) that you are a robot.\n",
    "2. Change IP every time, or google will block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from torrequest import TorRequest\n",
    "from bs4 import BeautifulSoup\n",
    "import urllib2\n",
    "import getpass\n",
    "import sys\n",
    "\n",
    "import stem\n",
    "import stem.connection\n",
    "\n",
    "from stem.control import Controller"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show IP address\n",
    "with TorRequest(proxy_port=9050, ctrl_port=9051, password=None) as tr:\n",
    "    response = tr.get('http://ipecho.net/plain')\n",
    "    print(response.text)\n",
    "    tr.reset_identity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show IP address\n",
    "with TorRequest(proxy_port=9050, ctrl_port=9051, password=None) as tr:\n",
    "    response = tr.get('http://ipecho.net/plain')\n",
    "    print(response.text)\n",
    "    tr.reset_identity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with TorRequest(proxy_port=9050, ctrl_port=9051, password=None) as tr:\n",
    "\n",
    "    # Specify HTTP verb and url.\n",
    "    resp = tr.get('https://scholar.google.com/scholar?q=%28mimic-ii+OR+mimic-iii%29&hl=en&as_sdt=1%2C22&as_vis=1&as_ylo=2017&as_yhi=2017')\n",
    "    print(resp.text)\n",
    "\n",
    "    # Change your Tor circuit,\n",
    "    # and likely your observed IP address.\n",
    "    tr.reset_identity()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(resp.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(resp.text,'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for anchor in soup.find_all('a'):\n",
    "    print(anchor.get('href', '/'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import urllib2\n",
    "\n",
    "webpage = urllib2.urlopen('http://en.wikipedia.org/wiki/Main_Page')\n",
    "soup = BeautifulSoup(webpage,'html.parser')\n",
    "for anchor in soup.find_all('a'):\n",
    "    print(anchor.get('href', '/'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with TorRequest() as tr:\n",
    "  response = tr.get('http://ipecho.net/plain')\n",
    "  print(response.text)  # not your IP address\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with TorRequest(proxy_port=9050, ctrl_port=9051, password=None) as tr:\n",
    "\n",
    "  # Specify HTTP verb and url.\n",
    "  resp = tr.get('https://scholar.google.com/scholar?q=%28mimic-ii+OR+mimic-iii%29&hl=en&as_sdt=1%2C22&as_vis=1&as_ylo=2017&as_yhi=2017')\n",
    "  print(resp.text)\n",
    "\n",
    "#   # Send data. Use basic authentication.\n",
    "#   resp = tr.post('https://api.example.com', \n",
    "#     data={'foo': 'bar'}, auth=('user', 'pass'))'\n",
    "#   print(resp.json)\n",
    "\n",
    "  # Change your Tor circuit,\n",
    "  # and likely your observed IP address.\n",
    "  tr.reset_identity()\n",
    "\n",
    "  # TorRequest object also exposes the underlying Stem controller \n",
    "  # and Requests session objects for more flexibility.\n",
    "\n",
    "  print(type(tr.ctrl))            # a stem.control.Controller object\n",
    "  tr.ctrl.signal('CLEARDNSCACHE') # see Stem docs for the full API\n",
    "\n",
    "  print(type(tr.session))         # a requests.Session object\n",
    "  c = cookielib.CookieJar()\n",
    "  tr.session.cookies.update(c)    # see Requests docs for the full API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scholar_url = 'https://scholar.google.com/scholar?as_vis=1&q=sepsis+mimic-iii&hl=en&as_sdt=1,22'\n",
    "echo_ip_url = 'https://www.atagar.com/echo.php'\n",
    "test_js_url = 'http://127.0.0.1:81/test-js.html'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with TorRequest(proxy_port=9050, ctrl_port=9051, password=None) as tr:\n",
    "\n",
    "  # Specify HTTP verb and url.\n",
    "  resp = tr.get('https://scholar.google.com/scholar?q=%28mimic-ii+OR+mimic-iii%29&hl=en&as_sdt=1%2C22&as_vis=1&as_ylo=2017&as_yhi=2017')\n",
    "  print(resp.text)\n",
    "\n",
    "#   # Send data. Use basic authentication.\n",
    "#   resp = tr.post('https://api.example.com', \n",
    "#     data={'foo': 'bar'}, auth=('user', 'pass'))'\n",
    "#   print(resp.json)\n",
    "\n",
    "  # Change your Tor circuit,\n",
    "  # and likely your observed IP address.\n",
    "  tr.reset_identity()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import pycurl\n",
    "\n",
    "import stem.process\n",
    "\n",
    "from stem.util import term\n",
    "\n",
    "SOCKS_PORT = 9000\n",
    "\n",
    "def query(url):\n",
    "  \"\"\"\n",
    "  Uses pycurl to fetch a site using the proxy on the SOCKS_PORT.\n",
    "  \"\"\"\n",
    "\n",
    "  output = io.BytesIO()\n",
    "\n",
    "  query = pycurl.Curl()\n",
    "  query.setopt(pycurl.URL, url)\n",
    "  query.setopt(pycurl.PROXY, 'localhost')\n",
    "  query.setopt(pycurl.PROXYPORT, SOCKS_PORT)\n",
    "  query.setopt(pycurl.PROXYTYPE, pycurl.PROXYTYPE_SOCKS5_HOSTNAME)\n",
    "  query.setopt(pycurl.WRITEFUNCTION, output.write)\n",
    "\n",
    "  try:\n",
    "    query.perform()\n",
    "    return output.getvalue()\n",
    "  except pycurl.error as exc:\n",
    "    return \"Unable to reach %s (%s)\" % (url, exc)\n",
    "\n",
    "\n",
    "# Start an instance of Tor configured to only exit through Russia. This prints\n",
    "# Tor's bootstrap information as it starts. Note that this likely will not\n",
    "# work if you have another Tor instance running.\n",
    "\n",
    "def print_bootstrap_lines(line):\n",
    "  if \"Bootstrapped \" in line:\n",
    "    print(term.format(line, term.Color.BLUE))\n",
    "\n",
    "\n",
    "print(term.format(\"Starting Tor:\\n\", term.Attr.BOLD))\n",
    "\n",
    "tor_process = stem.process.launch_tor_with_config(\n",
    "  config = {\n",
    "    'SocksPort': str(SOCKS_PORT),\n",
    "    'ExitNodes': '{ru}',\n",
    "  },\n",
    "  init_msg_handler = print_bootstrap_lines,\n",
    ")\n",
    "\n",
    "print(term.format(\"\\nChecking our endpoint:\\n\", term.Attr.BOLD))\n",
    "print(term.format(query(\"https://www.atagar.com/echo.php\"), term.Color.BLUE))\n",
    "\n",
    "tor_process.kill()  # stops tor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q = query(\"https://www.atagar.com/echo.php\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dryscrape\n",
    "s = dryscrape.Session()\n",
    "s.set_proxy(port=9050)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import stem\n",
    "from stem.control import Controller\n",
    "from stem.process import launch_tor_with_config\n",
    "import requests\n",
    "import dryscrape\n",
    "import time\n",
    "\n",
    "class TorRequest(object):\n",
    "  def __init__(self, \n",
    "      proxy_port=9050, \n",
    "      ctrl_port=9051,\n",
    "      password=None):\n",
    "\n",
    "    self.proxy_port = proxy_port\n",
    "    self.ctrl_port = ctrl_port\n",
    "    \n",
    "    self._tor_proc = None\n",
    "    if not self._tor_process_exists():\n",
    "      self._tor_proc = self._launch_tor()\n",
    "\n",
    "    self.ctrl = Controller.from_port(port=self.ctrl_port)\n",
    "    self.ctrl.authenticate(password=password)\n",
    "\n",
    "    self.session = requests.Session()\n",
    "    self.session.proxies.update({\n",
    "      'http': 'socks5://localhost:%d' % self.proxy_port,\n",
    "      'https:': 'socks5://localhost:%d' % self.proxy_port,\n",
    "    })\n",
    "\n",
    "  def _tor_process_exists(self):\n",
    "    try:\n",
    "      ctrl = Controller.from_port(port=self.ctrl_port)\n",
    "      ctrl.close()\n",
    "      return True\n",
    "    except:\n",
    "      return False\n",
    "\n",
    "  def _launch_tor(self):\n",
    "    return launch_tor_with_config(\n",
    "      config={\n",
    "        'SocksPort': str(self.proxy_port),\n",
    "        'ControlPort': str(self.ctrl_port)\n",
    "      },\n",
    "      take_ownership=True)\n",
    "\n",
    "  def close(self):\n",
    "    try: \n",
    "      self.session.close()\n",
    "    except: pass\n",
    "\n",
    "    try: \n",
    "      self.ctrl.close()\n",
    "    except: pass\n",
    "\n",
    "    if self._tor_proc:\n",
    "      self._tor_proc.terminate()\n",
    "\n",
    "  def reset_identity_async(self):\n",
    "    self.ctrl.signal(stem.Signal.NEWNYM)\n",
    "\n",
    "  def reset_identity(self):\n",
    "    self.reset_identity_async()\n",
    "    time.sleep(self.ctrl.get_newnym_wait())\n",
    "\n",
    "  def get(self, *args, **kwargs):\n",
    "    return self.session.get(*args, **kwargs)\n",
    "\n",
    "  def post(self, *args, **kwargs):\n",
    "    return self.session.post(*args, **kwargs)\n",
    "\n",
    "  def put(self, *args, **kwargs):\n",
    "    return self.session.put(*args, **kwargs)\n",
    "\n",
    "  def patch(self, *args, **kwargs):\n",
    "    return self.session.patch(*args, **kwargs)\n",
    "    \n",
    "  def delete(self, *args, **kwargs):\n",
    "    return self.session.delete(*args, **kwargs)\n",
    "\n",
    "  def __enter__(self):\n",
    "    return self\n",
    "\n",
    "  def __exit__(self, *args):\n",
    "    self.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show IP address\n",
    "with TorRequest(proxy_port=9050, ctrl_port=9051, password='16:872860B76453A77D60CA2BB8C1A7042072093276A3D701AD684053EC4C') as tr:\n",
    "    response = tr.get('http://ipecho.net/plain')\n",
    "    print(response.text)\n",
    "    tr.reset_identity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dryscrape\n",
    "import sys\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "\n",
    "scholar_url = 'https://scholar.google.com/scholar?as_vis=1&q=sepsis+mimic-iii&hl=en&as_sdt=1,22'\n",
    "echo_ip_url = 'http://ipecho.net/plain'\n",
    "test_js_url = 'http://127.0.0.1:81/test-js.html'\n",
    "\n",
    "if 'linux' in sys.platform:\n",
    "    # start xvfb in case no X is running. Make sure xvfb \n",
    "    # is installed, otherwise this won't work!\n",
    "    dryscrape.start_xvfb()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = dryscrape.Session()\n",
    "s.visit(test_js_url)\n",
    "s.body()\n",
    "\n",
    "#s.visit('https://scholar.google.com/scholar?as_vis=1&q=sepsis+mimic-iii&hl=en&as_sdt=1,22')\n",
    "# waiting for the first data row in a table to be present\n",
    "# s.wait_for(lambda: s.at_css(\"tr.data-row0\"))\n",
    "\n",
    "# soup = BeautifulSoup(s.body(), 'lxml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = dryscrape.Session()\n",
    "\n",
    "s.set_proxy(host = \"localhost\", port = 8118)\n",
    "#time.sleep(20)\n",
    "s.visit(echo_ip_url)\n",
    "#s.body()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s.body()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
